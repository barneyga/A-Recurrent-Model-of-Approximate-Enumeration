
@article{mcclelland,
	title = {Can {Generic} {Neural} {Networks} {Estimate} {Numerosity} {Like} {Humans}?},
	abstract = {Researchers exploring mathematical abilities have proposed that humans and animals possess an approximate number system (ANS) that enables them to estimate numerosities in visual displays. Experimental data shows that estimation responses exhibit a constant coefficient of variation (CV: ratio of variability of the estimates to their mean) for numerosities larger than four, and a constant CV has been taken as a signature characteristic of the innate ANS. For numerosities up to four, however, humans often produce error-free responses, suggesting the presence of estimation mechanisms distinct from the ANS specialized for this ‘subitizing range’. We explored whether a constant CV might arise from learning in generic neural networks using widely-used neural network learning procedures. We find that our networks exhibit a flat CV for numerosities larger than 4, but do not do so robustly for smaller numerosities. Our findings are consistent with the idea that estimation for numbers larger than 4 may not require innate specialization for number, while also supporting the view that a process different from the one we model may underlie estimation responses for the smallest numbers.},
	language = {en},
	author = {Chen, Sharon Y and Fang, Mengting},
	pages = {6},
	file = {ChenZhouFangMcC18Estimation.pdf:C\:\\Users\\griff\\Zotero\\storage\\M3VZDC9L\\ChenZhouFangMcC18Estimation.pdf:application/pdf},
}


@article{accumulator,
	title = {A primarily serial, foveal accumulator underlies approximate numerical estimation},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1819956116},
	doi = {10.1073/pnas.1819956116},
	abstract = {The approximate number system (ANS) has attracted broad interest due to its potential importance in early mathematical development and the fact that it is conserved across species. Models of the ANS and behavioral measures of ANS acuity both assume that quantity estimation is computed rapidly and in parallel across an entire view of the visual scene. We present evidence instead that ANS estimates are largely the product of a serial accumulation mechanism operating across visual fixations. We used an eye-tracker to collect data on participants’ visual fixations while they performed quantity-estimation and -discrimination tasks. We were able to predict participants’ numerical estimates using their visual fixation data: As the number of dots fixated increased, mean estimates also increased, and estimation error decreased. A detailed model-based analysis shows that fixated dots contribute twice as much as peripheral dots to estimated quantities; people do not “double count” multiply fixated dots; and they do not adjust for the proportion of area in the scene that they have fixated. The accumulation mechanism we propose explains reported effects of display time on estimation and earlier findings of a bias to underestimate quantities.},
	language = {en},
	number = {36},
	urldate = {2021-07-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Cheyette, Samuel J. and Piantadosi, Steven T.},
	month = sep,
	year = {2019},
	pages = {17729--17734},
	file = {Cheyette and Piantadosi - 2019 - A primarily serial, foveal accumulator underlies a.pdf:C\:\\Users\\griff\\Zotero\\storage\\DWYAWQPW\\Cheyette and Piantadosi - 2019 - A primarily serial, foveal accumulator underlies a.pdf:application/pdf},
}

@misc{gilmore,
	title = {Sampling from the mental number line: {How} are approximate number system representations formed? {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {Sampling from the mental number line},
	url = {https://reader.elsevier.com/reader/sd/pii/S0010027713001157?token=3A20114D5ACD769D7141D164E2564075C65A641A8876F2365FD791D640E450E20D63E5A0EFB0DF32EB1A11CEE23ECBE4&originRegion=us-east-1&originCreation=20210704182937},
	language = {en},
	urldate = {2021-07-04},
	doi = {10.1016/j.cognition.2013.06.003},
	file = {Snapshot:C\:\\Users\\griff\\Zotero\\storage\\KCIDPWPS\\S0010027713001157.html:text/html;Accepted Version:C\:\\Users\\griff\\Zotero\\storage\\DKB5P2GD\\Sampling from the mental number line How are appr.pdf:application/pdf},
}

@article{unified,
	title = {A unified account of numerosity perception},
	volume = {4},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-020-00946-0},
	doi = {10.1038/s41562-020-00946-0},
	language = {en},
	number = {12},
	urldate = {2021-07-04},
	journal = {Nature Human Behaviour},
	author = {Cheyette, Samuel J. and Piantadosi, Steven T.},
	month = dec,
	year = {2020},
	pages = {1265--1272},
	file = {Cheyette and Piantadosi - 2020 - A unified account of numerosity perception.pdf:C\:\\Users\\griff\\Zotero\\storage\\5D7XBNHD\\Cheyette and Piantadosi - 2020 - A unified account of numerosity perception.pdf:application/pdf},
}

@article{draw,
	title = {{DRAW}: {A} {Recurrent} {Neural} {Network} {For} {Image} {Generation}},
	shorttitle = {{DRAW}},
	url = {http://arxiv.org/abs/1502.04623},
	abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
	urldate = {2021-07-11},
	journal = {arXiv:1502.04623 [cs]},
	author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1502.04623},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {Gregor et al_2015_DRAW.pdf:C\:\\Users\\griff\\Zotero\\storage\\B5SBFY6D\\Gregor et al_2015_DRAW.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\griff\\Zotero\\storage\\8QKH3P5K\\1502.html:text/html},
}
@article{merten,
	title = {A {Labeled}-{Line} {Code} for {Small} and {Large} {Numerosities} in the {Monkey} {Prefrontal} {Cortex}},
	volume = {27},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1056-07.2007},
	doi = {10.1523/JNEUROSCI.1056-07.2007},
	language = {en},
	number = {22},
	urldate = {2020-12-29},
	journal = {Journal of Neuroscience},
	author = {Nieder, A. and Merten, K.},
	month = may,
	year = {2007},
	pages = {5986--5993},
	file = {Full Text:C\:\\Users\\griff\\Zotero\\storage\\GVGYM6JN\\Nieder and Merten - 2007 - A Labeled-Line Code for Small and Large Numerositi.pdf:application/pdf},
}

@article{kutter,
	title = {Single {Neurons} in the {Human} {Brain} {Encode} {Numbers}},
	volume = {100},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627318307414},
	doi = {10.1016/j.neuron.2018.08.036},
	language = {en},
	number = {3},
	urldate = {2020-12-29},
	journal = {Neuron},
	author = {Kutter, Esther F. and Bostroem, Jan and Elger, Christian E. and Mormann, Florian and Nieder, Andreas},
	month = nov,
	year = {2018},
	pages = {753--761.e4},
	file = {Full Text:C\:\\Users\\griff\\Zotero\\storage\\TU4NRD4Y\\Kutter et al. - 2018 - Single Neurons in the Human Brain Encode Numbers.pdf:application/pdf},
}


@article{roitman,
	title = {Monotonic {Coding} of {Numerosity} in {Macaque} {Lateral} {Intraparietal} {Area}},
	volume = {5},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.0050208},
	doi = {10.1371/journal.pbio.0050208},
	language = {en},
	number = {8},
	urldate = {2020-12-31},
	journal = {PLoS Biology},
	author = {Roitman, Jamie D and Brannon, Elizabeth M and Platt, Michael L},
	editor = {Dehaene, Stanislas},
	month = jul,
	year = {2007},
	pages = {e208},
	file = {Full Text:C\:\\Users\\griff\\Zotero\\storage\\L856SCVC\\Roitman et al. - 2007 - Monotonic Coding of Numerosity in Macaque Lateral .pdf:application/pdf},
}

@article{zorzi,
	title = {Emergence of a 'visual number sense' in hierarchical generative models},
	volume = {15},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/nn.2996},
	doi = {10.1038/nn.2996},
	language = {en},
	number = {2},
	urldate = {2021-01-05},
	journal = {Nature Neuroscience},
	author = {Stoianov, Ivilin and Zorzi, Marco},
	month = feb,
	year = {2012},
	pages = {194--196},
	file = {nn.2996.pdf:C\:\\Users\\griff\\Downloads\\nn.2996.pdf:application/pdf},
}


@article{nasr,
	title = {Number detectors spontaneously emerge in a deep neural network designed for visual object recognition},
	volume = {5},
	issn = {2375-2548},
	url = {https://advances.sciencemag.org/lookup/doi/10.1126/sciadv.aav7903},
	doi = {10.1126/sciadv.aav7903},
	abstract = {Humans and animals have a “number sense,” an innate capability to intuitively assess the number of visual items in a set, its numerosity. This capability implies that mechanisms to extract numerosity indwell the brain’s visual system, which is primarily concerned with visual object recognition. Here, we show that network units tuned to abstract numerosity, and therefore reminiscent of real number neurons, spontaneously emerge in a biologically inspired deep neural network that was merely trained on visual object recognition. These numerosity-tuned units underlay the network’s number discrimination performance that showed all the characteristics of human and animal number discriminations as predicted by the Weber-Fechner law. These findings explain the spontaneous emergence of the number sense based on mechanisms inherent to the visual system.},
	language = {en},
	number = {5},
	urldate = {2021-01-08},
	journal = {Science Advances},
	author = {Nasr, Khaled and Viswanathan, Pooja and Nieder, Andreas},
	month = may,
	year = {2019},
	pages = {eaav7903},
	file = {Full Text:C\:\\Users\\griff\\Zotero\\storage\\IC2YFGHI\\Nasr et al. - 2019 - Number detectors spontaneously emerge in a deep ne.pdf:application/pdf},
}

@techreport{dewind,
	type = {preprint},
	title = {The number sense is an emergent property of a deep convolutional neural network trained for object recognition},
	url = {http://biorxiv.org/lookup/doi/10.1101/609347},
	abstract = {Summary
          
            Humans and many non-human animals have the “number sense,” an ability to estimate the number of items in a set without counting. This innate sense of number is hypothesized to provide a foundation for more complex numerical and mathematical concepts. Here I investigated whether we also share the number sense with a deep convolutional neural network (DCNN) trained for object recognition. These
            in silico
            networks have revolutionized machine learning over the last seven years, allowing computers to reach human-level performance on object recognition tasks for the first time. Their architecture is based on the structure of mammalian visual cortex, and after they are trained, they provide a highly predictive model of responses in primate visual cortex, suggesting deep homologies. I found that the DCNN demonstrates three key hallmarks of the number sense: numerosity-selective units (analogous to biological neurons), the behavioral ratio effect, and ordinality over representational space. Because the DCNN was not trained to enumerate, I conclude that the number sense is an emergent property of the network, the result of some combination of the network architecture and the constraint to develop the complex representational structure necessary for object recognition. By analogy I conclude that the number sense in animals was not necessarily the result of direct selective pressure to enumerate but might have “come for free” with the evolution of a complex visual system that evolved to identify objects and scenes in the real world.},
	language = {en},
	urldate = {2021-02-17},
	institution = {Animal Behavior and Cognition},
	author = {DeWind, Nicholas K.},
	month = apr,
	year = {2019},
	doi = {10.1101/609347},
	file = {DeWind_2019_The number sense is an emergent property of a deep convolutional neural network.pdf:C\:\\Users\\griff\\Zotero\\storage\\BQ2I2XYY\\DeWind_2019_The number sense is an emergent property of a deep convolutional neural network.pdf:application/pdf},
}

 @article{competence, title={Numerosity discrimination in deep neural networks: Initial competence, developmental refinement and experience statistics}, volume={23}, DOI={10.1111/desc.12940}, number={5}, journal={Developmental Science}, author={Testolin, Alberto and Zou, Will Y. and McClelland, James L.}, year={2020}} 

@misc{deepmind,
      title={Recurrent Models of Visual Attention}, 
      author={Volodymyr Mnih and Nicolas Heess and Alex Graves and Koray Kavukcuoglu},
      year={2014},
      eprint={1406.6247},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kevinzakka,
  author = {Kevin Zakka},
  title = {Recurrent Visual Attention},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/kevinzakka/recurrent-visual-attention}},
  commit = {a38ac8958ebf1c61a10c4d5320f1e31d3d0b73dd}
}

@article{EDRAM,
   title={Enriched Deep Recurrent Visual Attention Model for Multiple Object Recognition},
   url={http://dx.doi.org/10.1109/WACV.2017.113},
   DOI={10.1109/wacv.2017.113},
   journal={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
   publisher={IEEE},
   author={Ablavatski, Artsiom and Lu, Shijian and Cai, Jianfei},
   year={2017},
   month={Mar}
}

@misc{DRAM,
      title={Multiple Object Recognition with Visual Attention}, 
      author={Jimmy Ba and Volodymyr Mnih and Koray Kavukcuoglu},
      year={2015},
      eprint={1412.7755},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
